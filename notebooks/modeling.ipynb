{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5892b033",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81446a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "\n",
    "# remoção segura para deixar o log mais limpo\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde5257",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados para o modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fdb854",
   "metadata": {},
   "source": [
    "Leitura do dataset de features e covertendo os tipos dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0d3e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do dataset\n",
    "dataset_features_v1 = pd.read_csv('../data/processed/dataset_features_v1.csv')\n",
    "\n",
    "# Conversão dos tipos numéricos e categóricos\n",
    "dataset_features_v1 = dataset_features_v1.astype({\n",
    "    'ID_CLIENTE': 'int64',\n",
    "    'VALOR_A_PAGAR': 'float64',\n",
    "    'TAXA': 'float64',\n",
    "    'RENDA_MES_ANTERIOR': 'float64',\n",
    "    'NO_FUNCIONARIOS': 'int64',\n",
    "    'FLAG_PF': 'int64',\n",
    "    'DIAS_ATRASO': 'int64',\n",
    "    'TARGET_INADIMPLENCIA': 'int64',\n",
    "    'DIAS_ADIANTAMENTO': 'int64',\n",
    "    'TEMPO_DE_CASA_MESES': 'int64',\n",
    "    'PRAZO_PAGAMENTO_DIAS': 'int64',\n",
    "    'MES_SAFRA': 'int32',\n",
    "    'INADIMPLENCIAS_ANTERIORES': 'int64',\n",
    "    'SEGMENTO_INDUSTRIAL': 'category',\n",
    "    'DOMINIO_EMAIL': 'category',\n",
    "    'PORTE': 'category',\n",
    "    'CEP_2_DIG': 'category',\n",
    "})\n",
    "\n",
    "# Conversão de datas\n",
    "dataset_features_v1['SAFRA_REF'] = pd.to_datetime(\n",
    "    dataset_features_v1['SAFRA_REF'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "dataset_features_v1['DATA_EMISSAO_DOCUMENTO'] = pd.to_datetime(\n",
    "    dataset_features_v1['DATA_EMISSAO_DOCUMENTO'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "dataset_features_v1['DATA_PAGAMENTO'] = pd.to_datetime(\n",
    "    dataset_features_v1['DATA_PAGAMENTO'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "dataset_features_v1['DATA_VENCIMENTO'] = pd.to_datetime(\n",
    "    dataset_features_v1['DATA_VENCIMENTO'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "dataset_features_v1['DATA_CADASTRO'] = pd.to_datetime(\n",
    "    dataset_features_v1['DATA_CADASTRO'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e679a2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "aa5d105e-401b-443f-aab7-9aa8faf20011",
       "rows": [
        [
         "ID_CLIENTE",
         "int64"
        ],
        [
         "SAFRA_REF",
         "datetime64[ns]"
        ],
        [
         "DATA_EMISSAO_DOCUMENTO",
         "datetime64[ns]"
        ],
        [
         "DATA_PAGAMENTO",
         "datetime64[ns]"
        ],
        [
         "DATA_VENCIMENTO",
         "datetime64[ns]"
        ],
        [
         "VALOR_A_PAGAR",
         "float64"
        ],
        [
         "TAXA",
         "float64"
        ],
        [
         "RENDA_MES_ANTERIOR",
         "float64"
        ],
        [
         "NO_FUNCIONARIOS",
         "int64"
        ],
        [
         "DATA_CADASTRO",
         "datetime64[ns]"
        ],
        [
         "FLAG_PF",
         "int64"
        ],
        [
         "SEGMENTO_INDUSTRIAL",
         "category"
        ],
        [
         "DOMINIO_EMAIL",
         "category"
        ],
        [
         "PORTE",
         "category"
        ],
        [
         "CEP_2_DIG",
         "category"
        ],
        [
         "DIAS_ATRASO",
         "int64"
        ],
        [
         "TARGET_INADIMPLENCIA",
         "int64"
        ],
        [
         "DIAS_ADIANTAMENTO",
         "int64"
        ],
        [
         "TEMPO_DE_CASA_MESES",
         "int64"
        ],
        [
         "PRAZO_PAGAMENTO_DIAS",
         "int64"
        ],
        [
         "MES_SAFRA",
         "int32"
        ],
        [
         "INADIMPLENCIAS_ANTERIORES",
         "int64"
        ],
        [
         "ADIANTAMENTOS_ANTERIORES",
         "int64"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 23
       }
      },
      "text/plain": [
       "ID_CLIENTE                            int64\n",
       "SAFRA_REF                    datetime64[ns]\n",
       "DATA_EMISSAO_DOCUMENTO       datetime64[ns]\n",
       "DATA_PAGAMENTO               datetime64[ns]\n",
       "DATA_VENCIMENTO              datetime64[ns]\n",
       "VALOR_A_PAGAR                       float64\n",
       "TAXA                                float64\n",
       "RENDA_MES_ANTERIOR                  float64\n",
       "NO_FUNCIONARIOS                       int64\n",
       "DATA_CADASTRO                datetime64[ns]\n",
       "FLAG_PF                               int64\n",
       "SEGMENTO_INDUSTRIAL                category\n",
       "DOMINIO_EMAIL                      category\n",
       "PORTE                              category\n",
       "CEP_2_DIG                          category\n",
       "DIAS_ATRASO                           int64\n",
       "TARGET_INADIMPLENCIA                  int64\n",
       "DIAS_ADIANTAMENTO                     int64\n",
       "TEMPO_DE_CASA_MESES                   int64\n",
       "PRAZO_PAGAMENTO_DIAS                  int64\n",
       "MES_SAFRA                             int32\n",
       "INADIMPLENCIAS_ANTERIORES             int64\n",
       "ADIANTAMENTOS_ANTERIORES              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_features_v1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4dec0",
   "metadata": {},
   "source": [
    "#### Remoção de colunas irrelevantes\n",
    "\n",
    "Comecei eliminando colunas que não agregam valor ao modelo.\n",
    "\n",
    "* **ID_CLIENTE** é apenas um identificador único, não possui utilidade preditiva.\n",
    "* As colunas de datas brutas (**DATA_EMISSAO_DOCUMENTO**, **DATA_PAGAMENTO**, **DATA_VENCIMENTO**, **DATA_CADASTRO** e **SAFRA_REF**) foram removidas porque já foram utilizadas para criar variáveis derivadas mais informativas.\n",
    "\n",
    "#### Prevenção de data leakage\n",
    "\n",
    "* As variáveis **DIAS_ATRASO** e **DIAS_ADIANTAMENTO** foram retiradas porque só estariam disponíveis após o vencimento, ou seja, depois da ocorrência ou não da inadimplência.\n",
    "* Manter essas colunas geraria vazamento de informação (data leakage), prejudicando a generalização do modelo e provocando overfitting.\n",
    "\n",
    "#### Codificação de variáveis categóricas\n",
    "\n",
    "* As variáveis **SEGMENTO_INDUSTRIAL**, **PORTE** e **DOMINIO_EMAIL**, por serem categóricas com baixa cardinalidade, foram tratadas com One-Hot Encoding.\n",
    "* Já a variável **CEP_2_DIG** apresenta alta cardinalidade, e por isso utilizei Frequency Encoding — uma técnica que substitui cada categoria pela frequência relativa com que aparece na base de dados. Isso evita a explosão de colunas sem perder a representatividade da variável.\n",
    "\n",
    "#### Separação dos dados\n",
    "\n",
    "* Após o pré-processamento, separei as variáveis explicativas (**X**) da variável alvo (**TARGET_INADIMPLENCIA**).\n",
    "* Em seguida, fiz a divisão entre treino e teste com **train_test_split**, utilizando **stratify=y** para garantir a mesma proporção de inadimplentes em ambas as amostras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69376d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar colunas irrelevantes\n",
    "cols_drop = [\n",
    "    'ID_CLIENTE', 'DATA_EMISSAO_DOCUMENTO', 'DATA_PAGAMENTO',\n",
    "    'DATA_VENCIMENTO', 'DATA_CADASTRO', 'SAFRA_REF',\n",
    "    'DIAS_ATRASO', 'DIAS_ADIANTAMENTO', # Colunas que só são conhecidas depois do treinamento. São removidas para evitar data leakage\n",
    "]\n",
    "\n",
    "df = dataset_features_v1.drop(columns=cols_drop)\n",
    "\n",
    "\n",
    "# Aplicando One-Hot Encoding em variáveis categóricas de baixa cardinalidade\n",
    "df = pd.get_dummies(df, columns=['SEGMENTO_INDUSTRIAL', 'PORTE', 'DOMINIO_EMAIL'], drop_first=True)\n",
    "\n",
    "# Aplicando Frequency Encoding para variável com alta cardinalidade \n",
    "freq_map = df['CEP_2_DIG'].value_counts(normalize=True)\n",
    "df['CEP_2_DIG'] = df['CEP_2_DIG'].map(freq_map)\n",
    "\n",
    "\n",
    "X = df.drop(columns='TARGET_INADIMPLENCIA')\n",
    "y = df['TARGET_INADIMPLENCIA']\n",
    "\n",
    "# Divisão treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d24508",
   "metadata": {},
   "source": [
    "## Escolhendo o modelo preditor e seus parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f908fb9",
   "metadata": {},
   "source": [
    "Explicação do que eu estou fazendo aqui embaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df0681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando: XGBoost\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Treinando: RandomForest\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Treinando: LogisticRegression\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "Treinando: LightGBM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "# Para o XGBoost.\n",
    "#scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "modelos_parametros = {\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            #use_label_encoder=False,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [3, 5, 7, 10],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"clf__gamma\": [0, 1, 5],  # controle da complexidade\n",
    "            \"clf__min_child_weight\": [1, 5, 10],  # regularização de folhas\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [None, 10, 20],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "            \"clf__max_features\": ['sqrt', 'log2'],  # diversificação\n",
    "            \"clf__bootstrap\": [True, False]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42),\n",
    "        {\n",
    "            \"clf__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__num_leaves\": [31, 50, 100],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__min_child_samples\": [10, 20, 50],\n",
    "            \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# Validação cruzada e execução \n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = {}\n",
    "\n",
    "for nome, (modelo, grid_params) in modelos_parametros.items():\n",
    "    print(f\"\\nTreinando: {nome}\")\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"clf\", modelo)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=grid_params,\n",
    "        cv=cv,\n",
    "        scoring=\"roc_auc\",\n",
    "        verbose=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    resultados[nome] = {\n",
    "        \"Melhores parâmetros\": grid.best_params_,\n",
    "        \"ROC AUC (validação)\": grid.best_score_,\n",
    "        \"ROC AUC (teste)\": roc_auc_score(y_test, y_proba),\n",
    "        \"Classification Report\": classification_report(y_test, y_pred, output_dict=True),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, y_pred).tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Melhores parâmetros: {'clf__colsample_bytree': 0.8, 'clf__learning_rate': 0.1, 'clf__max_depth': 7, 'clf__n_estimators': 200, 'clf__subsample': 0.8}\n",
      "ROC AUC (validação): 0.9517\n",
      "ROC AUC (teste): 0.9604\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.977644  0.972352  0.974991  13672.000000\n",
      "1              0.674419  0.720331  0.696619   1087.000000\n",
      "accuracy       0.953791  0.953791  0.953791      0.953791\n",
      "macro avg      0.826031  0.846342  0.835805  14759.000000\n",
      "weighted avg   0.955311  0.953791  0.954489  14759.000000\n",
      "Confusion Matrix:\n",
      "[[13294   378]\n",
      " [  304   783]]\n",
      "\n",
      "=== RandomForest ===\n",
      "Melhores parâmetros: {'clf__max_depth': None, 'clf__n_estimators': 200}\n",
      "ROC AUC (validação): 0.961\n",
      "ROC AUC (teste): 0.9659\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.979844  0.974254  0.977041  13672.000000\n",
      "1              0.697854  0.747930  0.722025   1087.000000\n",
      "accuracy       0.957585  0.957585  0.957585      0.957585\n",
      "macro avg      0.838849  0.861092  0.849533  14759.000000\n",
      "weighted avg   0.959075  0.957585  0.958259  14759.000000\n",
      "Confusion Matrix:\n",
      "[[13320   352]\n",
      " [  274   813]]\n",
      "\n",
      "=== LogisticRegression ===\n",
      "Melhores parâmetros: {'clf__C': 10.0}\n",
      "ROC AUC (validação): 0.8598\n",
      "ROC AUC (teste): 0.8732\n",
      "Classification Report:\n",
      "              precision    recall  f1-score       support\n",
      "0              0.977414  0.841940  0.904633  13672.000000\n",
      "1              0.275319  0.755290  0.403539   1087.000000\n",
      "accuracy       0.835558  0.835558  0.835558      0.835558\n",
      "macro avg      0.626366  0.798615  0.654086  14759.000000\n",
      "weighted avg   0.925704  0.835558  0.867727  14759.000000\n",
      "Confusion Matrix:\n",
      "[[11511  2161]\n",
      " [  266   821]]\n",
      "\n",
      "=== LightGBM ===\n",
      "Melhores parâmetros: {'clf__learning_rate': 0.1, 'clf__n_estimators': 200, 'clf__num_leaves': 50}\n",
      "ROC AUC (validação): 0.9524\n",
      "ROC AUC (teste): 0.9607\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.975364  0.978789  0.977074  13672.00000\n",
      "1              0.720885  0.689052  0.704610   1087.00000\n",
      "accuracy       0.957450  0.957450  0.957450      0.95745\n",
      "macro avg      0.848125  0.833921  0.840842  14759.00000\n",
      "weighted avg   0.956622  0.957450  0.957007  14759.00000\n",
      "Confusion Matrix:\n",
      "[[13382   290]\n",
      " [  338   749]]\n"
     ]
    }
   ],
   "source": [
    "# Resultados Finais \n",
    "for nome, res in resultados.items():\n",
    "    print(f\"\\n=== {nome} ===\")\n",
    "    print(\"Melhores parâmetros:\", res[\"Melhores parâmetros\"])\n",
    "    print(\"ROC AUC (validação):\", round(res[\"ROC AUC (validação)\"], 4))\n",
    "    print(\"ROC AUC (teste):\", round(res[\"ROC AUC (teste)\"], 4))\n",
    "    print(\"Classification Report:\")\n",
    "    print(pd.DataFrame(res[\"Classification Report\"]).T)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(np.array(res[\"Confusion Matrix\"]))\n",
    "    #plotando a matriz de confusão \n",
    "    #cm = np.array(res[\"Confusion Matrix\"])\n",
    "    #disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    #disp.plot(cmap=\"plasma\", values_format=\"d\")\n",
    "    #plt.title(f\"Matriz de Confusão - {nome}\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ccd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testar esta gama de parâmetros mais tarde \n",
    "\n",
    "'''\n",
    "modelos_parametros = {\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [3, 5, 7, 10],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"clf__gamma\": [0, 1, 5],  # controle da complexidade\n",
    "            \"clf__min_child_weight\": [1, 5, 10],  # regularização de folhas\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__max_depth\": [None, 10, 20],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "            \"clf__max_features\": ['sqrt', 'log2'],  # diversificação\n",
    "            \"clf__bootstrap\": [True, False]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42),\n",
    "        {\n",
    "            \"clf__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200, 300],\n",
    "            \"clf__num_leaves\": [31, 50, 100],\n",
    "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"clf__min_child_samples\": [10, 20, 50],\n",
    "            \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4753d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Para o XGBoost.\\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\\n\\nmodelos_parametros = {\\n    \"XGBoost\": (\\n        XGBClassifier(\\n            objective=\"binary:logistic\",\\n            eval_metric=\"logloss\",\\n            #scale_pos_weight=scale_pos_weight, # ignorado pois já usamos SMOTE\\n            random_state=42\\n        ),\\n        {\\n            \"clf__n_estimators\": [100, 200],\\n            \"clf__max_depth\": [3, 5, 7],\\n            \"clf__learning_rate\": [0.01, 0.1],\\n            \"clf__subsample\": [0.8],\\n            \"clf__colsample_bytree\": [0.8]\\n        }\\n    ),\\n    \"RandomForest\": (\\n        RandomForestClassifier(random_state=42),\\n        {\\n            \"clf__n_estimators\": [100, 200],\\n            \"clf__max_depth\": [None, 10],\\n        }\\n    ),\\n    \"LogisticRegression\": (\\n        LogisticRegression(max_iter=1000, solver=\\'lbfgs\\', random_state=42),\\n        {\\n            \"clf__C\": [0.1, 1.0, 10.0]\\n        }\\n    ),\\n    \"LightGBM\": (\\n        LGBMClassifier(random_state=42, verbose=-1),\\n        {\\n            \"clf__n_estimators\": [100, 200],\\n            \"clf__num_leaves\": [31, 50],\\n            \"clf__learning_rate\": [0.01, 0.1]\\n        }\\n    )\\n}\\n\\n# Validação cruzada e execução \\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\nresultados = {}\\n\\nfor nome, (modelo, grid_params) in modelos_parametros.items():\\n    print(f\"\\nTreinando: {nome}\")\\n    pipeline = Pipeline(steps=[\\n        (\"scaler\", StandardScaler()),\\n        (\"smote\", SMOTE(random_state=42)),\\n        (\"clf\", modelo)\\n    ])\\n\\n    grid = GridSearchCV(\\n        pipeline,\\n        param_grid=grid_params,\\n        cv=cv,\\n        scoring=\"roc_auc\",\\n        verbose=1,\\n        n_jobs=-1\\n    )\\n\\n    grid.fit(X_train, y_train)\\n\\n    best_model = grid.best_estimator_\\n    y_pred = best_model.predict(X_test)\\n    y_proba = best_model.predict_proba(X_test)[:, 1]\\n\\n    resultados[nome] = {\\n        \"Melhores parâmetros\": grid.best_params_,\\n        \"ROC AUC (validação)\": grid.best_score_,\\n        \"ROC AUC (teste)\": roc_auc_score(y_test, y_proba),\\n        \"Classification Report\": classification_report(y_test, y_pred, output_dict=True),\\n        \"Confusion Matrix\": confusion_matrix(y_test, y_pred).tolist()\\n    }\\n\\n# Resultados Finais \\nfor nome, res in resultados.items():\\n    print(f\"\\n=== {nome} ===\")\\n    print(\"Melhores parâmetros:\", res[\"Melhores parâmetros\"])\\n    print(\"ROC AUC (validação):\", round(res[\"ROC AUC (validação)\"], 4))\\n    print(\"ROC AUC (teste):\", round(res[\"ROC AUC (teste)\"], 4))\\n    print(\"Classification Report:\")\\n    print(pd.DataFrame(res[\"Classification Report\"]).T)\\n    print(\"Confusion Matrix:\")\\n    print(np.array(res[\"Confusion Matrix\"])'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Para o XGBoost.\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "modelos_parametros = {\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            #scale_pos_weight=scale_pos_weight, # ignorado pois já usamos SMOTE\n",
    "            random_state=42\n",
    "        ),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [3, 5, 7],\n",
    "            \"clf__learning_rate\": [0.01, 0.1],\n",
    "            \"clf__subsample\": [0.8],\n",
    "            \"clf__colsample_bytree\": [0.8]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [None, 10],\n",
    "        }\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42),\n",
    "        {\n",
    "            \"clf__C\": [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42, verbose=-1),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__num_leaves\": [31, 50],\n",
    "            \"clf__learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# Validação cruzada e execução \n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = {}\n",
    "\n",
    "for nome, (modelo, grid_params) in modelos_parametros.items():\n",
    "    print(f\"\\nTreinando: {nome}\")\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"clf\", modelo)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=grid_params,\n",
    "        cv=cv,\n",
    "        scoring=\"roc_auc\",\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    resultados[nome] = {\n",
    "        \"Melhores parâmetros\": grid.best_params_,\n",
    "        \"ROC AUC (validação)\": grid.best_score_,\n",
    "        \"ROC AUC (teste)\": roc_auc_score(y_test, y_proba),\n",
    "        \"Classification Report\": classification_report(y_test, y_pred, output_dict=True),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, y_pred).tolist()\n",
    "    }\n",
    "\n",
    "# Resultados Finais \n",
    "for nome, res in resultados.items():\n",
    "    print(f\"\\n=== {nome} ===\")\n",
    "    print(\"Melhores parâmetros:\", res[\"Melhores parâmetros\"])\n",
    "    print(\"ROC AUC (validação):\", round(res[\"ROC AUC (validação)\"], 4))\n",
    "    print(\"ROC AUC (teste):\", round(res[\"ROC AUC (teste)\"], 4))\n",
    "    print(\"Classification Report:\")\n",
    "    print(pd.DataFrame(res[\"Classification Report\"]).T)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(np.array(res[\"Confusion Matrix\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597c2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menos agressivo, para somente testar a base de dados\n",
    "\n",
    "\n",
    "modelos_parametros = {\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            #scale_pos_weight=scale_pos_weight, # ignorado pois já usamos SMOTE\n",
    "            random_state=42\n",
    "        ),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [3, 5, 7],\n",
    "            \"clf__learning_rate\": [0.01, 0.1],\n",
    "            \"clf__subsample\": [0.8],\n",
    "            \"clf__colsample_bytree\": [0.8]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [None, 10],\n",
    "        }\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42),\n",
    "        {\n",
    "            \"clf__C\": [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42, verbose=-1),\n",
    "        {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__num_leaves\": [31, 50],\n",
    "            \"clf__learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3e9ce",
   "metadata": {},
   "source": [
    "primeiros resultados \n",
    "\n",
    "\n",
    "\n",
    "=== XGBoost ===\n",
    "Melhores parâmetros: {'clf__colsample_bytree': 0.8, 'clf__learning_rate': 0.1, 'clf__max_depth': 7, 'clf__n_estimators': 200, 'clf__subsample': 0.8}\n",
    "ROC AUC (validação): 0.9517\n",
    "ROC AUC (teste): 0.9604\n",
    "Classification Report:\n",
    "              precision    recall  f1-score       support\n",
    "0              0.977644  0.972352  0.974991  13672.000000\n",
    "1              0.674419  0.720331  0.696619   1087.000000\n",
    "accuracy       0.953791  0.953791  0.953791      0.953791\n",
    "macro avg      0.826031  0.846342  0.835805  14759.000000\n",
    "weighted avg   0.955311  0.953791  0.954489  14759.000000\n",
    "Confusion Matrix:\n",
    "[[13294   378]\n",
    " [  304   783]]\n",
    "\n",
    "=== RandomForest ===\n",
    "Melhores parâmetros: {'clf__max_depth': None, 'clf__n_estimators': 200}\n",
    "ROC AUC (validação): 0.961\n",
    "ROC AUC (teste): 0.9659\n",
    "Classification Report:\n",
    "              precision    recall  f1-score       support\n",
    "0              0.979844  0.974254  0.977041  13672.000000\n",
    "1              0.697854  0.747930  0.722025   1087.000000\n",
    "accuracy       0.957585  0.957585  0.957585      0.957585\n",
    "macro avg      0.838849  0.861092  0.849533  14759.000000\n",
    "weighted avg   0.959075  0.957585  0.958259  14759.000000\n",
    "Confusion Matrix:\n",
    "[[13320   352]\n",
    " [  274   813]]\n",
    "\n",
    "=== LogisticRegression ===\n",
    "Melhores parâmetros: {'clf__C': 10.0}\n",
    "ROC AUC (validação): 0.8598\n",
    "ROC AUC (teste): 0.8732\n",
    "Classification Report:\n",
    "              precision    recall  f1-score       support\n",
    "0              0.977414  0.841940  0.904633  13672.000000\n",
    "1              0.275319  0.755290  0.403539   1087.000000\n",
    "accuracy       0.835558  0.835558  0.835558      0.835558\n",
    "macro avg      0.626366  0.798615  0.654086  14759.000000\n",
    "weighted avg   0.925704  0.835558  0.867727  14759.000000\n",
    "Confusion Matrix:\n",
    "[[11511  2161]\n",
    " [  266   821]]\n",
    "\n",
    "=== LightGBM ===\n",
    "Melhores parâmetros: {'clf__learning_rate': 0.1, 'clf__n_estimators': 200, 'clf__num_leaves': 50}\n",
    "ROC AUC (validação): 0.9524\n",
    "ROC AUC (teste): 0.9607\n",
    "Classification Report:\n",
    "              precision    recall  f1-score      support\n",
    "0              0.975364  0.978789  0.977074  13672.00000\n",
    "1              0.720885  0.689052  0.704610   1087.00000\n",
    "accuracy       0.957450  0.957450  0.957450      0.95745\n",
    "macro avg      0.848125  0.833921  0.840842  14759.00000\n",
    "weighted avg   0.956622  0.957450  0.957007  14759.00000\n",
    "Confusion Matrix:\n",
    "[[13382   290]\n",
    " [  338   749]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
